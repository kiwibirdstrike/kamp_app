# llm/ollama_client.py
from __future__ import annotations
import requests
from sklearn import tree as sktree

# ===== ê³ ì • ì„¤ì • =====
OLLAMA_BASE = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:3b"

# ===== í•œêµ­ì–´ í•´ì„¤ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ =====
_PROMPT_TEMPLATE_KOR = """[ëª©í‘œ]
ë‹¹ì‹ ì€ ì œì¡° ê³µì • ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì´ì Explainable AI í•´ì„ ì—”ì§„ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ëª¨ë¸ í•™ìŠµ ê²°ê³¼(Feature Importance, Tree Structure, Decision Rules, Failure Rules)ë¥¼ ì „ë¬¸ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë¶„ì„ ëŒ€ìƒì€ ì£¼ë¡œ ì œì¡° ê³µì •ì˜ í’ˆì§ˆ ì˜ˆì¸¡, ë¶ˆëŸ‰ ì›ì¸ ì§„ë‹¨ ëª¨ë¸ì´ë©°, Decision Treeê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.

[ê·œì¹™]
ë‹¹ì‹ ì˜ í•´ì„ì€ ë‹¤ìŒ ê¸°ì¤€ì„ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤.
ë³€ìˆ˜ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë³€ìˆ˜ëª…ì— ëŒ€í•œ ì‹¤ì œ ì˜ë¯¸ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤. (ì˜ˆ: ì••ì¶œê¸° ëª¨í„°ì†ë„(EX1.MD_PV))
ê° íŒŒíŠ¸ë³„ Plotì„ ì¶œë ¥í•˜ê³  ê·¸ ë°‘ì— í•´ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

1. Feature Importance í•´ì„
   - ìƒìœ„ ì¤‘ìš” ë³€ìˆ˜ë“¤ì´ ì˜ë¯¸í•˜ëŠ” ë¬¼ë¦¬ì /ê³µì •ì  ì˜ë¯¸ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
   - ì¤‘ìš”ë„ê°€ ë†’ì€ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ì„œ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•©ë‹ˆë‹¤.

2. Tree Structure í•´ì„
   - ëª¨ë¸ì´ ì£¼ìš”í•˜ê²Œ ë¶„ê¸°í•˜ëŠ” ë³€ìˆ˜ ì¡°í•©ì„ ìš”ì•½í•©ë‹ˆë‹¤.
   - íŠ¸ë¦¬ì˜ ìƒìœ„ ë…¸ë“œì™€ í•˜ìœ„ ë…¸ë“œê°€ ì–´ë–¤ ê³µì • ì¡°ê±´(ì˜ˆ: ì˜¨ë„, ì••ë ¥, ì‹œê°„)ì„ êµ¬ë¶„í•˜ëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.
   - íŠ¸ë¦¬ì˜ ì „ì²´ êµ¬ì¡°ê°€ "ì–´ë–¤ ì¡°ê±´ì„ ê¸°ì¤€ìœ¼ë¡œ Pass/Failì„ êµ¬ë¶„í•˜ëŠ”ì§€"ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

3. Decision Rules í•´ì„
   - ëŒ€í‘œì ì¸ ì˜ì‚¬ê²°ì • ê·œì¹™ì„ ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ "If-Then" ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
   - ê° ê·œì¹™ì˜ ì˜ë¯¸(ì˜ˆ: â€œì˜¨ë„ê°€ 850ë„ ì´ìƒì´ë©´ ë¶ˆëŸ‰ë¥ ì´ ê¸‰ê²©íˆ ì¦ê°€í•œë‹¤â€)ë¥¼ ê¸°ìˆ í•©ë‹ˆë‹¤.
   - ì¤‘ìš”í•œ ê·œì¹™ì€ ë°œìƒ ë¹ˆë„ì™€ ì •í™•ë„(coverage, confidence) ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  ìˆœìœ„ë¥¼ ë‘¡ë‹ˆë‹¤.

4. Failure Rules í•´ì„
   - ë¶ˆëŸ‰(Fail) ë°œìƒ êµ¬ê°„ì˜ íŠ¹ì§•ì ì¸ ì¡°ê±´ì„ ìš”ì•½í•©ë‹ˆë‹¤.
   - ì–´ë–¤ ë³€ìˆ˜ ì¡°í•©ì´ í’ˆì§ˆ ë¶ˆëŸ‰ì„ ê°•í•˜ê²Œ ìœ ë°œí•˜ëŠ”ì§€, ì •ìƒê³¼ì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.
   - ê°€ëŠ¥í•œ ê°œì„  ë°©í–¥(ì˜ˆ: â€œëƒ‰ê° ì‹œê°„ ì¡°ì •â€, â€œê°€ì—´ ì˜¨ë„ ê· ì¼í™”â€)ì„ ì œì•ˆí•©ë‹ˆë‹¤.

5. ì „ë¬¸ì„± ê¸°ì¤€
   - í•´ì„ì€ ê¸°ìˆ  ì—”ì§€ë‹ˆì–´ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ê³¼í•™ì  ê·¼ê±°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
   - ë¶ˆí™•ì‹¤í•œ í•´ì„ì€ â€œê°€ëŠ¥ì„±ì´ ìˆë‹¤â€, â€œì¶”ì •ëœë‹¤â€ ë“±ì˜ í‘œí˜„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
   - ì„¤ëª…ì€ í•œê¸€ë¡œ ëª…í™•í•˜ê³ , ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

[ëª¨ë¸ ì¶œë ¥ ìš”ì•½(í…ìŠ¤íŠ¸)]
{MODEL_SUMMARY}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ë¡œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ â€œFeature Importance/Tree Structure/Decision Rules/Failure Rulesâ€ í•´ì„ì„ ì‘ì„±í•˜ì‹œì˜¤.
ê° íŒŒíŠ¸ëŠ” ì„¹ì…˜ ì•„ì´ì½˜(ğŸ”/ğŸŒ²/âš™ï¸/ğŸ’¥)ê³¼ í•¨ê»˜ ì‹œì‘í•˜ê³ , í•­ëª©ë§ˆë‹¤ ì§§ì€ êµµì€ ì†Œì œëª©ì„ í¬í•¨í•´ ê°€ë…ì„±ì„ ë†’ì´ì‹œì˜¤.
"""

def build_kor_explanation_prompt(base_summary: str) -> str:
    """ì•±ì—ì„œ ì „ë‹¬ëœ ëª¨ë¸ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ í•´ì„¤ í…œí”Œë¦¿ì— ì£¼ì…."""
    return _PROMPT_TEMPLATE_KOR.replace("{MODEL_SUMMARY}", base_summary)

def is_ollama_alive(base_url: str = OLLAMA_BASE) -> bool:
    try:
        r = requests.get(f"{base_url}/api/tags", timeout=5)
        return r.status_code == 200
    except Exception:
        return False

def is_ollama_model_available(model: str = OLLAMA_MODEL, base_url: str = OLLAMA_BASE) -> bool:
    try:
        r = requests.get(f"{base_url}/api/tags", timeout=10)
        r.raise_for_status()
        data = r.json()
        names = [m.get("name") or m.get("model") for m in data.get("models", [])]
        return model in names
    except Exception:
        return False

def call_llm_ollama(
    prompt: str,
    model: str = OLLAMA_MODEL,
    base_url: str = OLLAMA_BASE,
    max_tokens: int = 700,
    options: dict | None = None,
):
    """Ollama /api/generate í˜¸ì¶œ. options: temperature/top_p/num_predict ë“±."""
    try:
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.2,
                "top_p": 0.9,
                "num_predict": max_tokens,
            },
        }
        if options:
            payload["options"].update(options)

        r = requests.post(f"{base_url}/api/generate", json=payload, timeout=60)
        try:
            data = r.json()
        except Exception:
            data = {"raw": r.text}
        if r.status_code != 200:
            return f"[LLM] ì˜¤ë¥˜ {r.status_code}: {data}"
        return data.get("response", str(data))
    except Exception as e:
        return f"[LLM] í˜¸ì¶œ ì‹¤íŒ¨: {e}"

def build_tree_summary_for_llm(model, feat_names, thresholds_map, task, metrics_dict):
    """íŠ¸ë¦¬ ìš”ì•½ì„ LLM í”„ë¡¬í”„íŠ¸ë¡œ ë§Œë“¤ê¸° (ê·œì¹™ ë¯¸ë¦¬ë³´ê¸°, ë¶„ê¸° ì„ê³„ê°’ ì¼ë¶€ í¬í•¨)"""
    lines = []
    lines.append(f"Task: {task}")
    if metrics_dict:
        kv = ", ".join([f"{k}={v:.4f}" if isinstance(v, (float, int)) else f"{k}={v}" for k, v in metrics_dict.items()])
        lines.append(f"Metrics: {kv}")
    try:
        rules_text = sktree.export_text(model, feature_names=feat_names, max_depth=4)
        lines.append("Rules (depth<=4):\n" + rules_text)
    except Exception:
        pass
    used = {k: v for k, v in thresholds_map.items() if v}
    if used:
        for f, ths in list(used.items())[:10]:
            lines.append(f"Splits[{f}]: {ths[:10]}")
    return "\n".join(lines)
