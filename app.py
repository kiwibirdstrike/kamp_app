# app.py
from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import tree as sktree

from core.io_utils import try_read_csv, detect_time_columns
from core.metrics import compute_metrics_numeric
from core.plotting import plot_numeric_feature_with_thresholds
from ml.pipelines import (
    auto_feature_recommendations,
    get_feature_names_from_preprocessor,
    extract_numeric_split_thresholds,
)
from ml.train import build_tree_pipeline_and_train
from llm.ollama_client import (
    OLLAMA_BASE,
    OLLAMA_MODEL,
    is_ollama_alive,
    is_ollama_model_available,
    call_llm_ollama,
    build_tree_summary_for_llm,
    build_kor_explanation_prompt,
)

# ============================
# ê¸°ë³¸ ì„¤ì •
# ============================
st.set_page_config(page_title="ì†Œì„±ê°€ê³µë°ì´í„°ë¶„ì„", layout="wide")
st.title("ë°ì´í„° ì‹œê°í™” Â· ëª¨ë¸ í˜„ì¥ ì ìš© í•™ìŠµ ")

with st.expander("â„¹ï¸ ì‚¬ìš©ë²•", expanded=True):
    st.write(
        """
        - CSV ì—…ë¡œë“œ â†’ ì‹œê°„ì—´ ìë™ íƒì§€ â†’ ê³µì • ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€ìˆ˜ ë¶„í¬ë„ ì‹œê°í™”
        - ì‹œê°í™” í™”ë©´ì—ì„œ ê° ë³€ìˆ˜ í”Œë¡¯ ì˜†ì— í•´ë‹¹ ë³€ìˆ˜ì˜ â€˜ë¬´ì§ˆì„œë„(ë¶„í¬ì˜ ë„“ì´)â€™ì™€ â€˜ë¶ˆê· í˜•ë„(ë¶„í¬ì˜ ì¹˜ìš°ì¹¨)â€™ ì§€í‘œê°€ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.
        - ëª¨ë¸ í•™ìŠµ(ë¶„ë¥˜): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¡œ í•™ìŠµí•˜ê³  ì„±ëŠ¥, íŠ¸ë¦¬ êµ¬ì¡°(ê¹Šì´ ì¡°ì ˆ), ì˜í–¥ë ¥(Feature Importance)ì„ í™•ì¸í•©ë‹ˆë‹¤.
        - ë¶„ì„ í•´ì„¤: ì¤‘ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ TOP Nì˜ ì„ê³„ê°’ íˆìŠ¤í† ê·¸ë¨ì„ ë„“ê²Œ ë°°ì¹˜í•˜ê³ , ì „ë¬¸ í•œêµ­ì–´ í•´ì„¤ì„ ì œê³µí•©ë‹ˆë‹¤.
        """
    )

# ============================
# ìƒë‹¨ CSV ì—…ë¡œë”
# ============================
uploaded = st.file_uploader("CSV ì—…ë¡œë“œ", type=["csv"])

def _uploaded_sig(u) -> str | None:
    if u is None:
        return None
    try:
        return f"{u.name}:{u.size}"
    except Exception:
        return u.name

new_sig = _uploaded_sig(uploaded)
if "uploaded_sig" not in st.session_state:
    st.session_state.uploaded_sig = None
if new_sig != st.session_state.uploaded_sig:
    st.session_state.uploaded_sig = new_sig
    st.session_state.shape_shown_once = False
    st.session_state.trained = False
    st.session_state.pipe = None
    st.session_state.feature_importance_df = None
    st.session_state.rules_text = ""
    st.session_state.viz_page = 1  # ì‹œê°í™” í˜ì´ì§€ë„¤ì´ì…˜ ì´ˆê¸°í™”

# ============================
# NAV
# ============================
PAGES = ["ëŒ€ì‹œë³´ë“œ", "ì‹œê°í™”", "ëª¨ë¸ í•™ìŠµ", "ë¶„ì„ í•´ì„¤"]
if "page" not in st.session_state:
    st.session_state.page = PAGES[0]

nav_cols = st.columns(len(PAGES))
for i, name in enumerate(PAGES):
    if nav_cols[i].button(name, use_container_width=True, key=f"nav_{name}"):
        st.session_state.page = name
        st.rerun()

st.markdown("---")

# ============================
# ë°ì´í„° ë¡œë“œ
# ============================
if uploaded is None:
    st.info("â¬†ï¸ ìƒë‹¨ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

with st.spinner("CSV ë¡œë”© ì¤‘â€¦"):
    df = try_read_csv(uploaded)

if st.session_state.page == "ëŒ€ì‹œë³´ë“œ" and not st.session_state.get("shape_shown_once", False):
    st.success(f"ë¡œë“œ ì™„ë£Œ! shape = {df.shape}")
    st.session_state.shape_shown_once = True

# ì‹œê°í™” ìƒ˜í”Œ ì œí•œ
if df.shape[0] > 100_000:
    st.warning(f"í–‰ì´ {df.shape[0]:,}ê°œë¡œ í½ë‹ˆë‹¤. ì‹œê°í™”ëŠ” ì„±ëŠ¥ ë³´í˜¸ë¥¼ ìœ„í•´ ìƒìœ„ {100_000:,}í–‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    df_viz = df.head(100_000).copy()
else:
    df_viz = df.copy()

# ============================
# ì‹œê°„ì—´ ì²˜ë¦¬
# ============================
found_time_cols = detect_time_columns(df_viz)
if "selected_time_col" not in st.session_state:
    st.session_state.selected_time_col = (found_time_cols[0] if found_time_cols else None)

def _apply_time(df_src: pd.DataFrame) -> tuple[pd.DataFrame, str]:
    if st.session_state.selected_time_col:
        tcol = st.session_state.selected_time_col
        time_dt = pd.to_datetime(df_src[tcol], errors="coerce", infer_datetime_format=True)
        df2 = df_src.loc[time_dt.notna()].copy()
        df2["__time_dt__"] = time_dt.loc[time_dt.notna()]
        return df2, f"ğŸ•’ ì‚¬ìš© ì¤‘ì¸ ì‹œê°„ì—´: {tcol} (ìœ íš¨ í–‰ {df2.shape[0]:,}ê°œ)"
    else:
        df2 = df_src.reset_index(drop=False).rename(columns={"index": "__row__"})
        df2["__time_dt__"] = pd.to_datetime(df2["__row__"], unit="s", origin="unix")
        return df2, "ğŸ•’ ê°ì§€ëœ ì‹œê°„ì—´ì´ ì—†ì–´ í–‰ ìˆœì„œë¥¼ ì‹œê°„ì¶•ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."

df_viz_time, time_caption = _apply_time(df_viz)

# ============================
# ëŒ€ì‹œë³´ë“œ â€” êµ¬ì¡° + N/ê²°ì¸¡
# ============================
if st.session_state.page == "ëŒ€ì‹œë³´ë“œ":
    st.subheader("ë°ì´í„° êµ¬ì¡°(ìƒìœ„ 50í–‰)")
    if found_time_cols:
        st.selectbox(
            "ì‹œê°„ì—´ ì„ íƒ",
            options=found_time_cols,
            index=(found_time_cols.index(st.session_state.selected_time_col)
                   if st.session_state.selected_time_col in found_time_cols else 0),
            key="selected_time_col"
        )
    st.caption(time_caption)
    st.dataframe(df.head(50), use_container_width=True)

    st.subheader("ì—´ë³„ ìœ íš¨ê°œìˆ˜(N) Â· ê²°ì¸¡ ìˆ˜")
    counts = [{"ì—´": c, "ìœ íš¨ê°œìˆ˜(N)": int(df[c].notna().sum()), "ê²°ì¸¡": int(df[c].isna().sum())}
              for c in df.columns]
    counts_df = pd.DataFrame(counts).sort_values("ì—´").reset_index(drop=True)
    st.dataframe(counts_df, use_container_width=True)

# ============================
# ìœ í‹¸: Feature Importance ì§‘ê³„
# ============================
def aggregate_feature_importances(
    feature_names: list[str],
    importances: np.ndarray,
    numeric_features: list[str],
    categorical_features: list[str],
) -> pd.DataFrame:
    agg: dict[str, float] = {}
    for name, imp in zip(feature_names, importances):
        raw = None
        if name in numeric_features:
            raw = name
        else:
            for c in categorical_features:
                if name == c or name.startswith(c + "_"):
                    raw = c
                    break
        if raw is None:
            raw = name
        agg[raw] = agg.get(raw, 0.0) + float(imp)

    df_imp = pd.DataFrame({"feature": list(agg.keys()), "importance": list(agg.values())})
    df_imp.sort_values("importance", ascending=False, inplace=True, key=lambda s: np.round(s, 12))
    df_imp.reset_index(drop=True, inplace=True)
    return df_imp

# ============================
# ì‹œê°í™” â€” 4ê°œì”©, ë²„íŠ¼ í˜ì´ì§€ë„¤ì´ì…˜(ê³ ìœ  key + ì¦‰ì‹œ rerun), ê° í”Œë¡¯ ì˜† ì§€í‘œ
# ============================
if st.session_state.page == "ì‹œê°í™”":
    st.subheader("ê³µì • ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€ìˆ˜ ë¶„í¬ë„")
    st.caption("ê° ë³€ìˆ˜ í”Œë¡¯ ì˜¤ë¥¸ìª½ì— â€˜ë¬´ì§ˆì„œë„(ë¶„í¬ì˜ ë„“ì´) / ë¶ˆê· í˜•ë„(ê°’ì˜ ì¹˜ìš°ì¹¨)â€™ ì§€í‘œê°€ í•¨ê»˜ í‘œì‹œë©ë‹ˆë‹¤.")

    num_cols_all = df_viz_time.select_dtypes(include=[np.number]).columns.tolist()
    num_cols_all = [c for c in num_cols_all if c != "__row__"]

    if not num_cols_all:
        st.info("ì—°ì†í˜•(ìˆ˜ì¹˜í˜•) ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        met_df = compute_metrics_numeric(df=df_viz_time)[["column", "entropy", "gini"]]
        metrics_all = {row["column"]: (row["entropy"], row["gini"]) for _, row in met_df.iterrows()}

        page_size = 4
        total_pages = (len(num_cols_all) + page_size - 1) // page_size
        if "viz_page" not in st.session_state:
            st.session_state.viz_page = 1

        # --- ë²„íŠ¼ í˜ì´ì§€ë„¤ì´ì…˜: key_prefix + ì¦‰ì‹œ rerun ---
        def _render_pagination(key_prefix: str):
            curr = st.session_state.viz_page
            window = 10
            start_p = max(1, curr - window // 2)
            end_p = min(total_pages, start_p + window - 1)
            cols = st.columns(2 + (end_p - start_p + 1))
            # Prev
            if cols[0].button("â—€ Prev", use_container_width=True, disabled=(curr == 1), key=f"{key_prefix}_prev"):
                st.session_state.viz_page = max(1, curr - 1)
                st.rerun()
            # Numbers
            for i, p in enumerate(range(start_p, end_p + 1), start=1):
                label = f"[{p}]" if p == curr else f"{p}"
                if cols[i].button(label, use_container_width=True, key=f"{key_prefix}_page_{p}"):
                    st.session_state.viz_page = p
                    st.rerun()
            # Next
            if cols[-1].button("Next â–¶", use_container_width=True, disabled=(curr == total_pages), key=f"{key_prefix}_next"):
                st.session_state.viz_page = min(total_pages, curr + 1)
                st.rerun()

        _render_pagination("viz_top")

        start = (st.session_state.viz_page - 1) * page_size
        batch_cols = num_cols_all[start:start + page_size]

        for col in batch_cols:
            left, right = st.columns([3, 1], vertical_alignment="center")
            with left:
                fig, ax = plt.subplots(figsize=(10, 3))
                y = pd.to_numeric(df_viz_time[col], errors="coerce")
                m = df_viz_time["__time_dt__"].notna() & y.notna()
                ax.plot(df_viz_time.loc[m, "__time_dt__"], y.loc[m], linewidth=1)
                ax.set_title(col, fontsize=11)
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                plt.close(fig)
            with right:
                entropy, gini = metrics_all.get(col, (np.nan, np.nan))
                st.markdown(
                    f"**ë¬´ì§ˆì„œë„**: `{entropy:.4f}`  \n**ë¶ˆê· í˜•ë„**: `{gini:.4f}`"
                )

        _render_pagination("viz_bottom")

# ============================
# ëª¨ë¸ í•™ìŠµ â€” ë¶„ë¥˜
# ============================
if st.session_state.page == "ëª¨ë¸ í•™ìŠµ":
    st.header("ëª¨ë¸ í•™ìŠµ")

    cols_all = df.columns.tolist()
    exclude_cols_for_target = set(found_time_cols or []) | {"__time_dt__", "__row__"}
    available_targets = [c for c in cols_all if c not in exclude_cols_for_target]

    if not available_targets:
        st.warning("íƒ€ê¹ƒ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. (ì‹œê°„/ë³´ì¡° ì»¬ëŸ¼ë§Œ ì¡´ì¬)")
    else:
        target_col = st.selectbox("íƒ€ê¹ƒ ë³€ìˆ˜ ì„ íƒ", options=available_targets)

        n_y_na = df[target_col].isna().sum()
        if n_y_na > 0:
            st.info(f"íƒ€ê¹ƒ {target_col} ê²°ì¸¡ {n_y_na:,}ê°œ í–‰ì€ í•™ìŠµ ì „ ìë™ ì œê±°í•©ë‹ˆë‹¤.")

        task = "classification"

        recommended, drop_reasons = auto_feature_recommendations(df, target_col, found_time_cols)
        all_candidates = [c for c in df.columns if c not in set(found_time_cols or []) | {target_col, "__time_dt__", "__row__"}]

        with st.expander("ë¶„ì„ì„ ìœ„í•´ ì¡°ì • ê°€ëŠ¥í•œ ë³€ìˆ˜ë“¤ì„ ì„ íƒí•˜ì„¸ìš”", expanded=True):
            if drop_reasons:
                dr = pd.DataFrame({"column": list(drop_reasons.keys()), "reason": list(drop_reasons.values())})
                st.caption("ìë™ ì œì™¸ ì‚¬ìœ ")
                st.dataframe(dr, use_container_width=True)
            features_selected = st.multiselect("í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜", options=all_candidates, default=recommended)
            if not features_selected:
                st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ í”¼ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

        st.subheader("ëª¨ë¸ ì„¤ì •")
        complexity = st.slider("ëª¨ë¸ ë³µì¡ë„ (1=ë‹¨ìˆœ, 10=ë³µì¡)", 1, 10, 6)
        test_size = st.slider("ê²€ì¦ ë¹„ìœ¨(test_size)", 0.05, 0.5, 0.2, step=0.05)
        random_state = st.number_input("random_state", value=42)

        do_train = st.button("í•™ìŠµ ì‹¤í–‰", type="primary")
        if "trained" not in st.session_state:
            st.session_state.trained = False

        if do_train:
            if not features_selected:
                st.error("í•™ìŠµí•  í”¼ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()

            with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘â€¦"):
                result = build_tree_pipeline_and_train(
                    df=df,
                    target_col=target_col,
                    features_selected=features_selected,
                    task=task,
                    complexity=int(complexity),
                    test_size=float(test_size),
                    random_state=int(random_state),
                )

            if result.get("warning"):
                st.warning(result["warning"])
                st.stop()

            st.session_state.trained = True
            st.session_state.pipe = result["pipe"]
            st.session_state.task = task
            st.session_state.metrics_dict = result["metrics_dict"]
            st.session_state.cm_df = result["cm_df"]
            st.session_state.feat_names = get_feature_names_from_preprocessor(
                st.session_state.pipe.named_steps["preprocess"]
            )
            st.session_state.numeric_features = result["numeric_features"]
            st.session_state.categorical_features = [
                c for c in features_selected if c not in st.session_state.numeric_features
            ]
            st.session_state.thresholds_map = extract_numeric_split_thresholds(
                st.session_state.pipe.named_steps["model"],
                st.session_state.feat_names,
                st.session_state.numeric_features,
            )
            st.session_state.df_train_sample = result["df_train_sample"]

            try:
                st.session_state.rules_text = sktree.export_text(
                    st.session_state.pipe.named_steps["model"],
                    feature_names=st.session_state.feat_names,
                    max_depth=4,
                )
            except Exception:
                st.session_state.rules_text = ""

            try:
                importances = st.session_state.pipe.named_steps["model"].feature_importances_
                fi_df = aggregate_feature_importances(
                    feature_names=st.session_state.feat_names,
                    importances=importances,
                    numeric_features=st.session_state.numeric_features,
                    categorical_features=st.session_state.categorical_features,
                )
                st.session_state.feature_importance_df = fi_df
            except Exception:
                st.session_state.feature_importance_df = pd.DataFrame(columns=["feature", "importance"])

        if st.session_state.get("trained", False):
            st.subheader("í‰ê°€ ê²°ê³¼")
            st.write(st.session_state.metrics_dict)
            if st.session_state.cm_df is not None:
                st.write("í˜¼ë™í–‰ë ¬")
                st.dataframe(st.session_state.cm_df, use_container_width=True)

            st.subheader("íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™”")
            plot_depth = st.slider("í‘œì‹œí•  íŠ¸ë¦¬ ê¹Šì´", 1, 10, 4, key="tree_plot_depth")
            fig, ax = plt.subplots(figsize=(24, 12))
            model = st.session_state.pipe.named_steps["model"]
            sktree.plot_tree(
                model,
                feature_names=st.session_state.feat_names,
                class_names=None,
                filled=True,
                rounded=True,
                max_depth=int(st.session_state.tree_plot_depth),
                fontsize=6,
                ax=ax,
            )
            st.pyplot(fig)
            plt.close(fig)

            st.subheader("ì˜í–¥ë ¥ (Feature Importance)")
            if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
                st.dataframe(st.session_state.feature_importance_df, use_container_width=True)
            else:
                st.info("ì˜í–¥ë ¥ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ============================
# ë¶„ì„ í•´ì„¤ â€” TOP N ì„ê³„ê°’ 3ì—´ + í•œêµ­ì–´ í•´ì„¤
# ============================
if st.session_state.page == "ë¶„ì„ í•´ì„¤":
    st.subheader("ë¶„ì„ í•´ì„¤")

    if not st.session_state.get("trained", False):
        st.warning("ë¨¼ì € â€˜ëª¨ë¸ í•™ìŠµâ€™ í˜ì´ì§€ì—ì„œ í•™ìŠµì„ ì™„ë£Œí•´ ì£¼ì„¸ìš”.")
    else:
        alive = is_ollama_alive()
        avail = is_ollama_model_available()
        if not alive:
            st.error("Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PowerShellì—ì„œ `ollama serve` ë˜ëŠ” ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        elif not avail:
            st.error(f"Ollamaì— ëª¨ë¸ '{OLLAMA_MODEL}' ì´(ê°€) ì—†ìŠµë‹ˆë‹¤. ì½˜ì†”ì—ì„œ `ollama pull {OLLAMA_MODEL}` ì‹¤í–‰ í›„ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
        else:
            top_n = st.slider("ìƒìœ„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°œìˆ˜(ì„ê³„ê°’ ì‹œê°í™”)", 3, 12, 6, 1)

            st.subheader("ìˆ˜ì¹˜í˜• í”¼ì²˜ ë¶„ê¸° ì„ê³„ê°’ (TOP N)")
            thresholds_map = st.session_state.thresholds_map
            df_train_sample = st.session_state.df_train_sample
            num_feats = st.session_state.numeric_features or []

            top_numeric = []
            if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
                fi = st.session_state.feature_importance_df
                fi_num = fi[fi["feature"].isin(num_feats)].head(int(top_n))
                top_numeric = fi_num["feature"].tolist()

            if not top_numeric:
                st.info("ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆ˜ì¹˜í˜• í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                cols = st.columns(3)  # 3ì—´ ë°°ì¹˜
                for i, f in enumerate(top_numeric):
                    ths = thresholds_map.get(f, [])
                    with cols[i % 3]:
                        if ths:
                            plot_numeric_feature_with_thresholds(df_train_sample, f, ths, bins=40)
                        else:
                            st.caption(f"- {f}: íŠ¸ë¦¬ ë¶„ê¸° ì„ê³„ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
                st.caption("ì„¸ë¡œì„ ì€ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•œ ë¶„ê¸° ì„ê³„ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„í¬ì˜ ë´‰ìš°ë¦¬ë¥¼ ê°€ë¥´ëŠ” ì„ê³„ê°’ì¼ìˆ˜ë¡ ì˜í–¥ë ¥ì´ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            if st.button("í•´ì„¤ ë³´ê¸°"):
                model = st.session_state.pipe.named_steps["model"]
                base_summary = build_tree_summary_for_llm(
                    model=model,
                    feat_names=st.session_state.feat_names,
                    thresholds_map=thresholds_map,
                    task="classification",
                    metrics_dict=st.session_state.metrics_dict,
                )
                if st.session_state.get("rules_text"):
                    base_summary += "\n\nRules (depth<=4):\n" + st.session_state.rules_text
                if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
                    top15 = st.session_state.feature_importance_df.head(15)
                    base_summary += "\n\nTop-15 Feature Importance:\n" + "\n".join(
                        f"- {row.feature}: {row.importance:.4f}" for _, row in top15.iterrows()
                    )

                prompt = build_kor_explanation_prompt(base_summary)
                with st.spinner("í•´ì„¤ ìƒì„± ì¤‘â€¦"):
                    resp = call_llm_ollama(
                        prompt,
                        options={"temperature": 0.2, "top_p": 0.9, "num_predict": 900},
                    )
                st.markdown(resp)
