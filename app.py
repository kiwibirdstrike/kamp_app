# app.py
from __future__ import annotations
import io
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import tree as sktree

from core.io_utils import try_read_csv, detect_time_columns
from core.metrics import compute_metrics_numeric
from core.plotting import plot_numeric_feature_with_thresholds
from ml.pipelines import (
    auto_feature_recommendations,
    get_feature_names_from_preprocessor,
    extract_numeric_split_thresholds,
)
from ml.train import build_tree_pipeline_and_train

# OpenAI API ì‚¬ìš©
from llm.openai_client import (
    OPENAI_MODEL,
    is_openai_configured,
    call_llm_openai,
)
# í”„ë¡¬í”„íŠ¸ ë¹Œë”ëŠ” ê¸°ì¡´ ê²ƒ ì¬ì‚¬ìš©(ê°™ì€ í•¨ìˆ˜ëª… ìœ ì§€)
from llm.ollama_client import (
    build_tree_summary_for_llm,
    build_kor_explanation_prompt,
)

# ============================
# ê¸°ë³¸ ì„¤ì •
# ============================
st.set_page_config(page_title="ì†Œì„±ê°€ê³µë°ì´í„°ë¶„ì„", layout="wide")
st.title("ë°ì´í„° ì‹œê°í™” Â· ëª¨ë¸ í˜„ì¥ ì ìš© í•™ìŠµ ")

with st.expander("â„¹ï¸ ì‚¬ìš©ë²•", expanded=True):
    st.write(
        """
        - CSV ì—…ë¡œë“œ â†’ ì‹œê°„ì—´ ìë™ íƒì§€ â†’ ê³µì • ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€ìˆ˜ ë¶„í¬ë„ ì‹œê°í™”
        - ì‹œê°í™” í™”ë©´ì—ì„œ ê° ë³€ìˆ˜ í”Œë¡¯ ì˜†ì— í•´ë‹¹ ë³€ìˆ˜ì˜ â€˜ë¬´ì§ˆì„œë„(ë¶„í¬ì˜ ë„“ì´)â€™ì™€ â€˜ë¶ˆê· í˜•ë„(ë¶„í¬ì˜ ì¹˜ìš°ì¹¨)â€™ ì§€í‘œê°€ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.
        - ëª¨ë¸ í•™ìŠµ(ë¶„ë¥˜; y=passorfail ë‚´ë¶€ ê³ ì •): ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¡œ í•™ìŠµí•˜ê³ , ê²°ê³¼ëŠ” **ë¶„ì„ í•´ì„¤** íƒ­ì—ì„œ í™•ì¸í•©ë‹ˆë‹¤.
        - ë¶„ì„ í•´ì„¤: ì¤‘ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ TOP Nì˜ ì„ê³„ê°’ íˆìŠ¤í† ê·¸ë¨ê³¼ ì „ë¬¸ í•œêµ­ì–´ í•´ì„¤(LLM)ì´ ì œê³µë©ë‹ˆë‹¤.
        """
    )

# ============================
# ìƒë‹¨ CSV ì—…ë¡œë”
# ============================
uploaded = st.file_uploader("CSV ì—…ë¡œë“œ", type=["csv"])

def _uploaded_sig(u) -> str | None:
    if u is None:
        return None
    try:
        return f"{u.name}:{u.size}"
    except Exception:
        return u.name

new_sig = _uploaded_sig(uploaded)
if "uploaded_sig" not in st.session_state:
    st.session_state.uploaded_sig = None
if new_sig != st.session_state.uploaded_sig:
    st.session_state.uploaded_sig = new_sig
    st.session_state.shape_shown_once = False
    st.session_state.trained = False
    st.session_state.pipe = None
    st.session_state.feature_importance_df = None
    st.session_state.rules_text = ""
    st.session_state.viz_page = 1  # ì‹œê°í™” í˜ì´ì§€ë„¤ì´ì…˜ ì´ˆê¸°í™”
    st.session_state.lag_info_note = "0 (ë¯¸ì ìš©)"

# ============================
# NAV
# ============================
PAGES = ["ëŒ€ì‹œë³´ë“œ", "ì‹œê°í™”", "ëª¨ë¸ í•™ìŠµ", "ë¶„ì„ í•´ì„¤"]
if "page" not in st.session_state:
    st.session_state.page = PAGES[0]

nav_cols = st.columns(len(PAGES))
for i, name in enumerate(PAGES):
    if nav_cols[i].button(name, use_container_width=True, key=f"nav_{name}"):
        st.session_state.page = name
        st.rerun()

st.markdown("---")

# ============================
# ë°ì´í„° ë¡œë“œ
# ============================
if uploaded is None:
    st.info("â¬†ï¸ ìƒë‹¨ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

with st.spinner("CSV ë¡œë”© ì¤‘â€¦"):
    df = try_read_csv(uploaded)

if st.session_state.page == "ëŒ€ì‹œë³´ë“œ" and not st.session_state.get("shape_shown_once", False):
    st.success(f"ë¡œë“œ ì™„ë£Œ! shape = {df.shape}")
    st.session_state.shape_shown_once = True

# ì‹œê°í™” ìƒ˜í”Œ ì œí•œ
if df.shape[0] > 100_000:
    st.warning(f"í–‰ì´ {df.shape[0]:,}ê°œë¡œ í½ë‹ˆë‹¤. ì‹œê°í™”ëŠ” ì„±ëŠ¥ ë³´í˜¸ë¥¼ ìœ„í•´ ìƒìœ„ {100_000:,}í–‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    df_viz = df.head(100_000).copy()
else:
    df_viz = df.copy()

# ============================
# ì‹œê°„ì—´ ì²˜ë¦¬
# ============================
found_time_cols = detect_time_columns(df_viz)
if "selected_time_col" not in st.session_state:
    st.session_state.selected_time_col = (found_time_cols[0] if found_time_cols else None)

def _apply_time(df_src: pd.DataFrame) -> tuple[pd.DataFrame, str]:
    if st.session_state.selected_time_col:
        tcol = st.session_state.selected_time_col
        time_dt = pd.to_datetime(df_src[tcol], errors="coerce", infer_datetime_format=True)
        df2 = df_src.loc[time_dt.notna()].copy()
        df2["__time_dt__"] = time_dt.loc[time_dt.notna()]
        return df2, f"ğŸ•’ ì‚¬ìš© ì¤‘ì¸ ì‹œê°„ì—´: {tcol} (ìœ íš¨ í–‰ {df2.shape[0]:,}ê°œ)"
    else:
        df2 = df_src.reset_index(drop=False).rename(columns={"index": "__row__"})
        df2["__time_dt__"] = pd.to_datetime(df2["__row__"], unit="s", origin="unix")
        return df2, "ğŸ•’ ê°ì§€ëœ ì‹œê°„ì—´ì´ ì—†ì–´ í–‰ ìˆœì„œë¥¼ ì‹œê°„ì¶•ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."

df_viz_time, time_caption = _apply_time(df_viz)

# ============================
# ëŒ€ì‹œë³´ë“œ â€” êµ¬ì¡° + N/ê²°ì¸¡
# ============================
if st.session_state.page == "ëŒ€ì‹œë³´ë“œ":
    st.subheader("ë°ì´í„° êµ¬ì¡°(ìƒìœ„ 50í–‰)")
    if found_time_cols:
        st.selectbox(
            "ì‹œê°„ì—´ ì„ íƒ",
            options=found_time_cols,
            index=(found_time_cols.index(st.session_state.selected_time_col)
                   if st.session_state.selected_time_col in found_time_cols else 0),
            key="selected_time_col"
        )
    st.caption(time_caption)
    st.dataframe(df.head(50), use_container_width=True)

    st.subheader("ì—´ë³„ ìœ íš¨ê°œìˆ˜(N) Â· ê²°ì¸¡ ìˆ˜")
    counts = [{"ì—´": c, "ìœ íš¨ê°œìˆ˜(N)": int(df[c].notna().sum()), "ê²°ì¸¡": int(df[c].isna().sum())}
              for c in df.columns]
    counts_df = pd.DataFrame(counts).sort_values("ì—´").reset_index(drop=True)
    st.dataframe(counts_df, use_container_width=True)

# ============================
# ìœ í‹¸: Feature Importance ì§‘ê³„
# ============================
def aggregate_feature_importances(
    feature_names: list[str],
    importances: np.ndarray,
    numeric_features: list[str],
    categorical_features: list[str],
) -> pd.DataFrame:
    agg: dict[str, float] = {}
    for name, imp in zip(feature_names, importances):
        raw = None
        if name in numeric_features:
            raw = name
        else:
            for c in categorical_features:
                if name == c or name.startswith(c + "_"):
                    raw = c
                    break
        if raw is None:
            raw = name
        agg[raw] = agg.get(raw, 0.0) + float(imp)

    df_imp = pd.DataFrame({"feature": list(agg.keys()), "importance": list(agg.values())})
    df_imp.sort_values("importance", ascending=False, inplace=True, key=lambda s: np.round(s, 12))
    df_imp.reset_index(drop=True, inplace=True)
    return df_imp

# ============================
# ì‹œê°í™” â€” 4ê°œì”©, ë²„íŠ¼ í˜ì´ì§€ë„¤ì´ì…˜
# ============================
if st.session_state.page == "ì‹œê°í™”":
    st.subheader("ê³µì • ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€ìˆ˜ ë¶„í¬ë„")
    st.caption("ê° ë³€ìˆ˜ í”Œë¡¯ ì˜¤ë¥¸ìª½ì— â€˜ë¬´ì§ˆì„œë„(ë¶„í¬ì˜ ë„“ì´) / ë¶ˆê· í˜•ë„(ê°’ì˜ ì¹˜ìš°ì¹¨)â€™ ì§€í‘œê°€ í•¨ê»˜ í‘œì‹œë©ë‹ˆë‹¤.")

    num_cols_all = df_viz_time.select_dtypes(include=[np.number]).columns.tolist()
    num_cols_all = [c for c in num_cols_all if c != "__row__"]

    if not num_cols_all:
        st.info("ì—°ì†í˜•(ìˆ˜ì¹˜í˜•) ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        met_df = compute_metrics_numeric(df=df_viz_time)[["column", "entropy", "gini"]]
        metrics_all = {row["column"]: (row["entropy"], row["gini"]) for _, row in met_df.iterrows()}

        page_size = 4
        total_pages = (len(num_cols_all) + page_size - 1) // page_size
        if "viz_page" not in st.session_state:
            st.session_state.viz_page = 1

        def _render_pagination(key_prefix: str):
            curr = st.session_state.viz_page
            window = 10
            start_p = max(1, curr - window // 2)
            end_p = min(total_pages, start_p + window - 1)
            cols = st.columns(2 + (end_p - start_p + 1))
            if cols[0].button("â—€ Prev", use_container_width=True, disabled=(curr == 1), key=f"{key_prefix}_prev"):
                st.session_state.viz_page = max(1, curr - 1); st.rerun()
            for i, p in enumerate(range(start_p, end_p + 1), start=1):
                label = f"[{p}]" if p == curr else f"{p}"
                if cols[i].button(label, use_container_width=True, key=f"{key_prefix}_page_{p}"):
                    st.session_state.viz_page = p; st.rerun()
            if cols[-1].button("Next â–¶", use_container_width=True, disabled=(curr == total_pages), key=f"{key_prefix}_next"):
                st.session_state.viz_page = min(total_pages, curr + 1); st.rerun()

        _render_pagination("viz_top")

        start = (st.session_state.viz_page - 1) * page_size
        batch_cols = num_cols_all[start:start + page_size]

        for col in batch_cols:
            left, right = st.columns([3, 1], vertical_alignment="center")
            with left:
                fig, ax = plt.subplots(figsize=(10, 3))
                y = pd.to_numeric(df_viz_time[col], errors="coerce")
                m = df_viz_time["__time_dt__"].notna() & y.notna()
                ax.plot(df_viz_time.loc[m, "__time_dt__"], y.loc[m], linewidth=1)
                ax.set_title(col, fontsize=11)
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                plt.close(fig)
            with right:
                entropy, gini = metrics_all.get(col, (np.nan, np.nan))
                st.markdown(f"**ë¬´ì§ˆì„œë„**: `{entropy:.4f}`  \n**ë¶ˆê· í˜•ë„**: `{gini:.4f}`")

        _render_pagination("viz_bottom")

# ============================
# ëª¨ë¸ í•™ìŠµ â€” y=passorfail ê³ ì • + X ì‹œì°¨(í–‰=5ì´ˆ)
# ============================
if st.session_state.page == "ëª¨ë¸ í•™ìŠµ":
    st.header("ëª¨ë¸ í•™ìŠµ")

    TARGET_COL = "passorfail"
    if TARGET_COL not in df.columns:
        st.error("ë°ì´í„°ì— 'passorfail' íƒ€ê¹ƒ ì—´ì´ ì—†ìŠµë‹ˆë‹¤. ë¶ˆëŸ‰ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì—´ëª…ì„ 'passorfail'ë¡œ ë§ì¶°ì£¼ì„¸ìš”.")
        st.stop()
    target_col = TARGET_COL

    # X í›„ë³´ (íƒ€ê¹ƒ/ì‹œê°„/ë³´ì¡° ì œì™¸)
    exclude_cols_for_target = set(found_time_cols or []) | {"__time_dt__", "__row__", target_col}
    all_candidates = [c for c in df.columns if c not in exclude_cols_for_target]

    # ìë™ ì¶”ì²œ & ì‚¬ìš©ì ì„ íƒ
    recommended, drop_reasons = auto_feature_recommendations(df, target_col, found_time_cols)
    with st.expander("í•™ìŠµì— ì‚¬ìš©í•  ë³€ìˆ˜ ì„ íƒ", expanded=True):
        if drop_reasons:
            dr = pd.DataFrame({"column": list(drop_reasons.keys()), "reason": list(drop_reasons.values())})
            st.caption("ìë™ ì œì™¸ ì‚¬ìœ ")
            st.dataframe(dr, use_container_width=True)
        features_selected = st.multiselect(
            "ì…ë ¥ ë³€ìˆ˜(X) ì„ íƒ",
            options=all_candidates,
            default=[c for c in recommended if c in all_candidates]
        )
        if not features_selected:
            st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ í”¼ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

    # ---- í˜„ì¥ ë°˜ì‘ì‹œê°„(ì‹œì°¨) ì ìš© ----
    st.subheader("í˜„ì¥ ë°˜ì‘ì‹œê°„(ì‹œì°¨) ì ìš©")
    st.caption("í’ˆì§ˆ ê²°ê³¼ê°€ ì¼ì • ì‹œê°„ í›„ ë°˜ì˜ëœë‹¤ê³  ê°€ì •í•˜ì—¬, ì…ë ¥ ë³€ìˆ˜(X)ë¥¼ ê³¼ê±°ë¡œ ì´ë™ì‹œì¼œ í•™ìŠµí•©ë‹ˆë‹¤.")
    lag_mode = st.radio(
        "ì‹œì°¨ ë‹¨ìœ„ ì„ íƒ",
        ["ì—†ìŒ(ì‹¤ì‹œê°„ ê°€ì •)", "ë¶„ ë‹¨ìœ„(ì‹œê°„ì—´ ê¸°ì¤€)", "í–‰ ìˆ˜(ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€)"],
        index=0, horizontal=True
    )

    secs_per_row = 5.0  # í•œ í–‰ = 5ì´ˆ (ê³ ì •)
    lag_minutes = 0
    lag_rows_input = 0
    if lag_mode == "ë¶„ ë‹¨ìœ„(ì‹œê°„ì—´ ê¸°ì¤€)":
        lag_minutes = int(st.number_input("ì§€ì—° ì‹œê°„(ë¶„)", min_value=0, value=0, step=1,
                        help="í•œ í–‰ë‹¹ 5ì´ˆë¡œ í™˜ì‚°í•˜ì—¬ ë¶„ì„ í–‰ ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."))
        approx_rows = int(round((lag_minutes * 60) / secs_per_row)) if lag_minutes > 0 else 0
        st.caption(f"ê¸°ì¤€ ê°„ê²© = {secs_per_row:.2f}ì´ˆ/í–‰ â†’ {lag_minutes}ë¶„ â‰ˆ {approx_rows}í–‰ ì´ë™")
    elif lag_mode == "í–‰ ìˆ˜(ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€)":
        lag_rows_input = int(st.number_input("ì§€ì—° í–‰ ìˆ˜(ìƒ˜í”Œ ìˆ˜)", min_value=0, value=0, step=1,
                           help="ì‹œê°„ì—´ì´ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ê· ë“± ê°„ê²©ì´ ì•„ë‹ ë•Œ ì§ì ‘ í–‰ ìˆ˜ë¡œ ì‹œì°¨ë¥¼ ì§€ì •í•©ë‹ˆë‹¤."))

    # ---- ëª¨ë¸ ì„¤ì • ----
    st.subheader("ëª¨ë¸ ì„¤ì •")
    complexity = st.slider("ëª¨ë¸ ë³µì¡ë„ (1=ë‹¨ìˆœ, 10=ë³µì¡)", 1, 10, 6)
    test_size = st.slider("ê²€ì¦ ë¹„ìœ¨(test_size)", 0.05, 0.5, 0.2, step=0.05)
    random_state = st.number_input("random_state", value=42)

    do_train = st.button("í•™ìŠµ ì‹¤í–‰", type="primary")
    if "trained" not in st.session_state:
        st.session_state.trained = False

    if do_train:
        if not features_selected:
            st.error("í•™ìŠµí•  í”¼ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            st.stop()

        # ---- ì‹œì°¨ ì ìš©: Y(t) ~ X(t - lag)
        lag_rows = 0
        lag_info_note = "0 (ë¯¸ì ìš©)"
        if lag_mode == "ë¶„ ë‹¨ìœ„(ì‹œê°„ì—´ ê¸°ì¤€)" and lag_minutes > 0:
            lag_rows = int(round((lag_minutes * 60) / secs_per_row))
            lag_info_note = f"{lag_minutes}ë¶„ â‰ˆ {lag_rows}í–‰ (ê°„ê²©={secs_per_row:.2f}s/í–‰)"
        elif lag_mode == "í–‰ ìˆ˜(ìƒ˜í”Œ ìˆ˜ ê¸°ì¤€)" and lag_rows_input > 0:
            lag_rows = int(lag_rows_input)
            lag_info_note = f"{lag_rows}í–‰"

        df_for_train = df.copy()
        if lag_rows > 0:
            for c in features_selected:
                try:
                    df_for_train[c] = df_for_train[c].shift(lag_rows)
                except Exception:
                    df_for_train[c] = pd.Series(df_for_train[c]).shift(lag_rows)
            df_for_train = df_for_train.dropna(subset=features_selected + [target_col])
        else:
            if df_for_train[target_col].isna().any():
                df_for_train = df_for_train.dropna(subset=[target_col])

        with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘â€¦"):
            result = build_tree_pipeline_and_train(
                df=df_for_train,
                target_col=target_col,
                features_selected=features_selected,
                task="classification",             # ê³ ì •(ì´ì§„ ë¶ˆëŸ‰ì—¬ë¶€)
                complexity=int(complexity),
                test_size=float(test_size),
                random_state=int(random_state),
            )

        if result.get("warning"):
            st.warning(result["warning"])
            st.stop()

        st.session_state.trained = True
        st.session_state.pipe = result["pipe"]
        st.session_state.task = "classification"
        st.session_state.metrics_dict = result["metrics_dict"]
        st.session_state.cm_df = result["cm_df"]
        st.session_state.feat_names = get_feature_names_from_preprocessor(
            st.session_state.pipe.named_steps["preprocess"]
        )
        st.session_state.numeric_features = result["numeric_features"]
        st.session_state.categorical_features = [
            c for c in features_selected if c not in st.session_state.numeric_features
        ]
        st.session_state.thresholds_map = extract_numeric_split_thresholds(
            st.session_state.pipe.named_steps["model"],
            st.session_state.feat_names,
            st.session_state.numeric_features,
        )
        st.session_state.df_train_sample = result["df_train_sample"]
        st.session_state.lag_info_note = lag_info_note  # ë¶„ì„ í•´ì„¤ íƒ­ì—ì„œ ì•ˆë‚´ìš©

        try:
            st.session_state.rules_text = sktree.export_text(
                st.session_state.pipe.named_steps["model"],
                feature_names=st.session_state.feat_names,
                max_depth=4,
            )
        except Exception:
            st.session_state.rules_text = ""

        try:
            importances = st.session_state.pipe.named_steps["model"].feature_importances_
            fi_df = aggregate_feature_importances(
                feature_names=st.session_state.feat_names,
                importances=importances,
                numeric_features=st.session_state.numeric_features,
                categorical_features=st.session_state.categorical_features,
            )
            st.session_state.feature_importance_df = fi_df
        except Exception:
            st.session_state.feature_importance_df = pd.DataFrame(columns=["feature", "importance"])

        st.success("í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‘‰ â€˜ë¶„ì„ í•´ì„¤â€™ íƒ­ì—ì„œ ê²°ê³¼ì™€ í•´ì„¤ì„ í™•ì¸í•˜ì„¸ìš”.")

# ============================
# ë¶„ì„ í•´ì„¤ â€” ê²°ê³¼ ëª¨ì•„ë³´ê¸° + TOP N ì„ê³„ê°’ + í•œêµ­ì–´ í•´ì„¤
# ============================
if st.session_state.page == "ë¶„ì„ í•´ì„¤":
    st.subheader("ë¶„ì„ í•´ì„¤")

    if not st.session_state.get("trained", False):
        st.warning("ë¨¼ì € â€˜ëª¨ë¸ í•™ìŠµâ€™ í˜ì´ì§€ì—ì„œ í•™ìŠµì„ ì™„ë£Œí•´ ì£¼ì„¸ìš”.")
    else:
        # ---- ì‹œì°¨ ì ìš© ì •ë³´ ì•ˆë‚´ ----
        st.caption(f"ğŸ•’ í•™ìŠµ ì‹œ ì ìš©ëœ ì‹œì°¨: {st.session_state.get('lag_info_note', '0 (ë¯¸ì ìš©)')}")
        st.caption("â€» ì…ë ¥ ë³€ìˆ˜(X)ëŠ” ìœ„ ì‹œì°¨ë§Œí¼ ê³¼ê±°ë¡œ ì´ë™í•˜ì—¬ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ---- (1) í‰ê°€ ê²°ê³¼ & í˜¼ë™í–‰ë ¬ ----
        st.subheader("í‰ê°€ ê²°ê³¼")
        st.write(st.session_state.metrics_dict)
        if st.session_state.cm_df is not None:
            st.write("í˜¼ë™í–‰ë ¬")
            st.dataframe(st.session_state.cm_df, use_container_width=True)

        # ---- (2) íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” ----
        st.subheader("íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™”")
        plot_depth = st.slider("í‘œì‹œí•  íŠ¸ë¦¬ ê¹Šì´", 1, 10, 4, key="tree_plot_depth")
        fig, ax = plt.subplots(figsize=(24, 12))
        model = st.session_state.pipe.named_steps["model"]
        sktree.plot_tree(
            model,
            feature_names=st.session_state.feat_names,
            class_names=None,
            filled=True,
            rounded=True,
            max_depth=int(st.session_state.tree_plot_depth),
            fontsize=6,
            ax=ax,
        )
        st.pyplot(fig)
        plt.close(fig)

        # ---- (3) ì˜í–¥ë ¥ (Feature Importance) ----
        st.subheader("ì˜í–¥ë ¥ (Feature Importance)")
        if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
            st.dataframe(st.session_state.feature_importance_df, use_container_width=True)
        else:
            st.info("ì˜í–¥ë ¥ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ---- (4) ìˆ˜ì¹˜í˜• í”¼ì²˜ ë¶„ê¸° ì„ê³„ê°’ (TOP N) ----
        top_n = st.slider("ìƒìœ„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°œìˆ˜(ì„ê³„ê°’ ì‹œê°í™”)", 3, 12, 6, 1)

        st.subheader("ìˆ˜ì¹˜í˜• í”¼ì²˜ ë¶„ê¸° ì„ê³„ê°’ (TOP N)")
        thresholds_map = st.session_state.thresholds_map
        df_train_sample = st.session_state.df_train_sample
        num_feats = st.session_state.numeric_features or []

        top_numeric = []
        if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
            fi = st.session_state.feature_importance_df
            fi_num = fi[fi["feature"].isin(num_feats)].head(int(top_n))
            top_numeric = fi_num["feature"].tolist()

        if not top_numeric:
            st.info("ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆ˜ì¹˜í˜• í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            cols = st.columns(3)
            for i, f in enumerate(top_numeric):
                ths = thresholds_map.get(f, [])
                with cols[i % 3]:
                    if ths:
                        plot_numeric_feature_with_thresholds(df_train_sample, f, ths, bins=40)
                    else:
                        st.caption(f"- {f}: íŠ¸ë¦¬ ë¶„ê¸° ì„ê³„ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.caption("ì„¸ë¡œì„ ì€ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•œ ë¶„ê¸° ì„ê³„ê°’ì…ë‹ˆë‹¤.")

        # ---- (5) LLM í•œêµ­ì–´ í•´ì„¤ ----
        if not is_openai_configured():
            st.info(
                "LLM í•´ì„¤ì„ ë³´ë ¤ë©´ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
                "ë¡œì»¬: `.streamlit/secrets.toml` ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì„¤ì •\n"
                "ë°°í¬: Streamlit Community Cloudì˜ Settings â†’ Secretsì— í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
            )
        else:
            if st.button("í•´ì„¤ ë³´ê¸°"):
                model = st.session_state.pipe.named_steps["model"]
                base_summary = build_tree_summary_for_llm(
                    model=model,
                    feat_names=st.session_state.feat_names,
                    thresholds_map=st.session_state.thresholds_map,
                    task="classification",
                    metrics_dict=st.session_state.metrics_dict,
                )
                if st.session_state.get("rules_text"):
                    base_summary += "\n\nRules (depth<=4):\n" + st.session_state.rules_text
                if st.session_state.feature_importance_df is not None and not st.session_state.feature_importance_df.empty:
                    top15 = st.session_state.feature_importance_df.head(15)
                    base_summary += "\n\nTop-15 Feature Importance:\n" + "\n".join(
                        f"- {row.feature}: {row.importance:.4f}" for _, row in top15.iterrows()
                    )

                prompt = build_kor_explanation_prompt(base_summary)
                with st.spinner("í•´ì„¤ ìƒì„± ì¤‘â€¦"):
                    resp = call_llm_openai(
                        prompt,
                        options={
                            "model": OPENAI_MODEL,
                            "temperature": 0.2,
                            "top_p": 0.9,
                            "max_tokens": 2000,
                        },
                    )
                st.markdown(resp)
